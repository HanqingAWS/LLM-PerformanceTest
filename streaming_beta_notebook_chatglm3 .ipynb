{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c261e5f4-17a8-40da-beb9-599f1717e0fe",
   "metadata": {},
   "source": [
    "### 1. 安装HuggingFace 并下载模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02785614-9268-41c8-85a5-d579490edbbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.188.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.196.0.tar.gz (916 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m916.9/916.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.28.57)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.24.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.23.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.18.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.9.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.57 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.31.57)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.57->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.196.0-py2.py3-none-any.whl size=1223208 sha256=4c5d02c4e8e6431f85b59bb51f39ed4fb70d2845c403efdf79c68e76b1a44845\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/86/1b/11b1150764a78929af99b11b7789b8f3ed340d2c31d425cfe2\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.188.0\n",
      "    Uninstalling sagemaker-2.188.0:\n",
      "      Successfully uninstalled sagemaker-2.188.0\n",
      "Successfully installed sagemaker-2.196.0\n",
      "Collecting boto\n",
      "  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: boto\n",
      "Successfully installed boto-2.49.0\n",
      "Requirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.31.57)\n",
      "Collecting botocore\n",
      "  Obtaining dependency information for botocore from https://files.pythonhosted.org/packages/27/3f/310eb7e4f1e8668e4d660e04b0ee1fac738eb2bb6e07a779dd7802d70aa3/botocore-1.31.74-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.31.74-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore) (1.26.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Downloading botocore-1.31.74-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.57\n",
      "    Uninstalling botocore-1.31.57:\n",
      "      Successfully uninstalled botocore-1.31.57\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.29.57 requires botocore==1.31.57, but you have botocore 1.31.74 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed botocore-1.31.74\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub -Uqq\n",
    "!pip install -U sagemaker\n",
    "!pip install -U boto\n",
    "!pip install -U botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d846ed0-24cb-45d3-be44-42a28bf5ce72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install sagemaker pip --upgrade  --quiet\n",
    "# Note the following may error depending on which awscli is installed in your jupyter kernel, \n",
    "# but that is ok \n",
    "#%pip install ./code/botocore-1.29.157-py3-none-any.whl ./code/boto3-1.26.157-py3-none-any.whl --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6daddee-833f-4755-9541-b5e1ea675024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ./LLM_chatglm3_stream_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6bd7ee-16a3-4f5a-8857-8bbba83eb9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_model_path = Path(\"./LLM_chatglm3_stream_model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"THUDM/chatglm3-6b\"\n",
    "commit_hash = \"fc3235f807ef5527af598c05f04f2ffd17f48bab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e8abc5-a58e-40e2-b1e6-fbf48307c716",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681b5ffeadd348239452fe575562def5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa6ce3b61fd4e54a4dea802926da93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ef5527af598c05f04f2ffd17f48bab/README.md:   0%|          | 0.00/5.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5becf975dde84ac3b4d157538b694775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)7af598c05f04f2ffd17f48bab/.gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324ee2281cc54555b9dfb566ef3fd8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)5527af598c05f04f2ffd17f48bab/config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5963dfc22560467a9d7761c22ddb1db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)27af598c05f04f2ffd17f48bab/MODEL_LICENSE:   0%|          | 0.00/4.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264890dc94794c60873d8528127c28f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)8c05f04f2ffd17f48bab/modeling_chatglm.py:   0%|          | 0.00/55.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafa1be865744bc68e90d5083664d126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)04f2ffd17f48bab/configuration_chatglm.py:   0%|          | 0.00/2.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76898119e213468698e6663df6addda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a559d55f2347ec8447a9206cacfc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9d51c22f1b4e958be2f5ebaad5e23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e4b528ae0041b6b31231c04fd975e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982d2ea0c43747c7b1c6a99046931cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e400e55f15043aa9e1c0f0dadd4beb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ec38addcea4955941d07a332f81cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c480b5b7feb0402bb275dcd5711fe6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ffd17f48bab/pytorch_model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1c9c78650a48d1a3405f5e3d266505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)af598c05f04f2ffd17f48bab/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004946791dbc403ab19a6392848b84fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)f04f2ffd17f48bab/tokenization_chatglm.py:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea77b7ae7cf4701a2fa96ba8b09bf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e975afbe062345188a874b3db7930432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)05f04f2ffd17f48bab/tokenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(repo_id=model_name, cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d666c79-b039-4258-ac3b-46b19e63c3b8",
   "metadata": {},
   "source": [
    "### 2. 把模型拷贝到S3为后续部署做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9431deb-6359-442d-847b-1563f8dd3854",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40dd8f16-ae7c-48bf-8e52-1a15425fa74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_code_prefix: LLM-RAG/workshop/LLM_chatglm3_stream_deploy_code\n",
      "model_snapshot_path: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab\n"
     ]
    }
   ],
   "source": [
    "s3_model_prefix = \"LLM-RAG/workshop/LLM_chatglm3_stream_model\"  # folder where model checkpoint will go\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "s3_code_prefix = \"LLM-RAG/workshop/LLM_chatglm3_stream_deploy_code\"\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067292c9-c066-4649-a61f-b460a24da584",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/README.md to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/README.md\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/config.json to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/config.json\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/MODEL_LICENSE to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/MODEL_LICENSE\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/configuration_chatglm.py to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/configuration_chatglm.py\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/modeling_chatglm.py to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/modeling_chatglm.py\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/.gitattributes to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/.gitattributes\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/pytorch_model-00004-of-00007.bin to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/pytorch_model-00004-of-00007.bin\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/pytorch_model-00003-of-00007.bin to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/pytorch_model-00003-of-00007.bin\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/pytorch_model.bin.index.json to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/pytorch_model.bin.index.json\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/pytorch_model-00001-of-00007.bin to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/pytorch_model-00001-of-00007.bin\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/quantization.py to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/quantization.py\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/tokenizer_config.json to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/tokenizer_config.json\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/tokenization_chatglm.py to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/tokenization_chatglm.py\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/tokenizer.model to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/tokenizer.model\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/pytorch_model-00002-of-00007.bin to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/pytorch_model-00002-of-00007.bin\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/pytorch_model-00005-of-00007.bin to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/pytorch_model-00005-of-00007.bin\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/pytorch_model-00007-of-00007.bin to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/pytorch_model-00007-of-00007.bin\n",
      "upload: LLM_chatglm3_stream_model/models--THUDM--chatglm3-6b/snapshots/fc3235f807ef5527af598c05f04f2ffd17f48bab/pytorch_model-00006-of-00007.bin to s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/pytorch_model-00006-of-00007.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://{bucket}/{s3_model_prefix}\n",
    "!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b70c3-90f1-4175-95bf-568bafbcd383",
   "metadata": {},
   "source": [
    "### 3. 模型部署准备（entrypoint脚本，容器镜像，服务配置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d60730-528b-4707-9189-6bf6f5cad754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\n",
    ")\n",
    "\n",
    "#中国区需要替换为下面的image_uri\n",
    "# inference_image_uri = (\n",
    "#     f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.21.0-deepspeed0.8.3-cu117\"\n",
    "# )\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d771bdb-11d2-45d2-9bef-face29221838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p LLM_chatglm3_stream_deploy_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e5348ecb-43df-4094-97d8-a6723004862a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LLM_chatglm3_stream_deploy_code/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_chatglm3_stream_deploy_code/model.py\n",
    "from djl_python import Input, Output\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "STOP_flag = \"[DONE]\"\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "DEVICE_ID = \"0\"\n",
    "CUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n",
    "def torch_gc():\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.cuda.device(CUDA_DEVICE):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            \n",
    "def load_model(properties):\n",
    "    global tokenizer,model\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, trust_remote_code=True)\n",
    "   \n",
    "    model = AutoModel.from_pretrained(model_location, trust_remote_code=True).half().cuda()\n",
    "    \n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def stream_items(prompt, history, max_length, top_p, temperature):\n",
    "    global model, tokenizer\n",
    "    size = 0\n",
    "    response = \"\"\n",
    "    for response, history in model.stream_chat(tokenizer, prompt, history=history, max_length=max_length, top_p=top_p,\n",
    "                                               temperature=temperature):\n",
    "        this_response = response[size:]\n",
    "        history = [list(h) for h in history]\n",
    "        size = len(response)\n",
    "        stream_buffer = { \"outputs\":this_response}\n",
    "        yield stream_buffer\n",
    "    ## stop\n",
    "    # yield {\"query\": prompt, \"outputs\": STOP_flag, \"response\": response, \"history\": history, \"finished\": True}\n",
    "    \n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    input_sentences = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    "    history = data[\"history\"]\n",
    "    print(f'input prompt:{input_sentences}')    \n",
    "    outputs = Output()\n",
    "    outputs.add_property(\"content-type\", \"application/jsonlines\")\n",
    "    outputs.add_stream_content(stream_items(input_sentences,history=history,**params))\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60a9a063-43ed-4bf2-9f51-200bca51d477",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option.s3url ==> s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/\n"
     ]
    }
   ],
   "source": [
    "print(f\"option.s3url ==> s3://{bucket}/{s3_model_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d1e60-3914-4059-a08f-05ac26761165",
   "metadata": {},
   "source": [
    "#### Note: option.s3url 需要按照自己的账号进行修改, 可以拷贝上一个cell的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8996fe44-8e70-468b-abc1-38187cb33f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LLM_chatglm3_stream_deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_chatglm3_stream_deploy_code/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.enable_streaming=True\n",
    "option.predict_timeout=240\n",
    "option.s3url=s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67241ec-7b58-47d4-afd9-1d1745ddbbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-31 06:51:14    1.5 KiB .gitattributes\n",
      "2023-10-31 06:51:14    4.0 KiB MODEL_LICENSE\n",
      "2023-10-31 06:51:14    5.4 KiB README.md\n",
      "2023-10-31 06:51:14    1.3 KiB config.json\n",
      "2023-10-31 06:51:14    2.3 KiB configuration_chatglm.py\n",
      "2023-10-31 06:51:14   54.3 KiB modeling_chatglm.py\n",
      "2023-10-31 06:51:14    1.7 GiB pytorch_model-00001-of-00007.bin\n",
      "2023-10-31 06:51:18    1.8 GiB pytorch_model-00002-of-00007.bin\n",
      "2023-10-31 06:51:27    1.8 GiB pytorch_model-00003-of-00007.bin\n",
      "2023-10-31 06:51:19    1.7 GiB pytorch_model-00004-of-00007.bin\n",
      "2023-10-31 06:51:23    1.8 GiB pytorch_model-00005-of-00007.bin\n",
      "2023-10-31 06:52:26    1.8 GiB pytorch_model-00006-of-00007.bin\n",
      "2023-10-31 06:52:27 1004.0 MiB pytorch_model-00007-of-00007.bin\n",
      "2023-10-31 06:52:35   20.0 KiB pytorch_model.bin.index.json\n",
      "2023-10-31 06:52:36   14.3 KiB quantization.py\n",
      "2023-10-31 06:52:36   11.0 KiB tokenization_chatglm.py\n",
      "2023-10-31 06:52:36  994.5 KiB tokenizer.model\n",
      "2023-10-31 06:52:36  244 Bytes tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_model/ --human"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef22a2-27b9-4018-a46b-6a99b532512f",
   "metadata": {},
   "source": [
    "#### 注意: 必须把transformers升级到4.27.1以上，否则会出现 [Issue344](https://github.com/THUDM/ChatGLM-6B/issues/344)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b7e76c6-6dbc-47fc-9f47-4765c526ab76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing LLM_chatglm3_stream_deploy_code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_chatglm3_stream_deploy_code/requirements.txt\n",
    "transformers==4.30.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ae6734a-aacd-410d-818d-0a962697c3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘model.tar.gz’: No such file or directory\n",
      "LLM_chatglm3_stream_deploy_code/\n",
      "LLM_chatglm3_stream_deploy_code/requirements.txt\n",
      "LLM_chatglm3_stream_deploy_code/model.py\n",
      "LLM_chatglm3_stream_deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!cd LLM_chatglm3_stream_deploy_code && rm -rf \".ipynb_checkpoints\"\n",
    "!tar czvf model.tar.gz LLM_chatglm3_stream_deploy_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f77dc76-6d8c-4665-ba88-f03e887c136c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-044324713311/LLM-RAG/workshop/LLM_chatglm3_stream_deploy_code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5853daa-b8a3-4485-8c0a-64bf83e93a18",
   "metadata": {},
   "source": [
    "### 4. 创建模型 & 创建endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef974ca1-9638-45a8-9145-ea9d03b2b072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatglm-stream-2023-10-31-06-55-50-011\n",
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n",
      "Created Model: arn:aws:sagemaker:us-west-2:044324713311:model/chatglm-stream-2023-10-31-06-55-50-011\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "model_name = name_from_base(f\"chatglm-stream\") # Append a timestamp to the provided string\n",
    "print(model_name)\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "        'Environment': {\n",
    "            'SERVING_OPTS': '-Dai.djl.logging.level=DEBUG'\n",
    "        },\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "233bb3a4-d737-41ad-8fcc-7082c6278e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:044324713311:endpoint-config/chatglm-stream-2023-10-31-06-55-50-011-config',\n",
       " 'ResponseMetadata': {'RequestId': 'cbde5249-80f6-4bc2-a6d1-53fdc7e37fe4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'cbde5249-80f6-4bc2-a6d1-53fdc7e37fe4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '126',\n",
       "   'date': 'Tue, 31 Oct 2023 06:56:14 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "#Note: ml.g4dn.2xlarge 也可以选择\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 10*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "734a39b0-473e-4421-94c8-74d2b4105038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-west-2:044324713311:endpoint/chatglm-stream-2023-10-31-06-55-50-011-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262e826-a810-401d-a5a9-f62febb24e5f",
   "metadata": {},
   "source": [
    "#### 持续检测模型部署进度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08969928-6b9e-4d9c-a033-a31f5f77bdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m status)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     resp \u001b[38;5;241m=\u001b[39m sm_client\u001b[38;5;241m.\u001b[39mdescribe_endpoint(EndpointName\u001b[38;5;241m=\u001b[39mendpoint_name)\n\u001b[1;32m     10\u001b[0m     status \u001b[38;5;241m=\u001b[39m resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpointStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985b427-3959-46f7-9a50-5a2b45e2d513",
   "metadata": {},
   "source": [
    "### 5. 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e56bfdaa-3469-4784-aa8a-e32177cde3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.06 ms, sys: 0 ns, total: 7.06 ms\n",
      "Wall time: 12.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "parameters = {\n",
    "  \"max_length\": 2048,\n",
    "  \"temperature\": 0.01,\n",
    "  \"top_p\":0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "3a2b6b7b-2e54-4f83-9258-cfbb30cfc829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "endpoint_name=\"chatglm-stream-2023-10-31-06-55-50-011-endpoint\"\n",
    "class StreamScanner:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the InvokeEndpointWithResponseStream event stream. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'readlines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        \n",
    "    def readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "317bd924-f5bb-4533-b67f-6124b04350ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.32.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.32.1)\n",
      "Requirement already satisfied: accelerate==0.20.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.32.1) (4.65.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.20.3) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.20.3) (2.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.1) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.1) (4.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.32.1) (3.0.9)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.32.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.32.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.32.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.32.1) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.32.1 accelerate==0.20.3\n",
    "!pip install accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "150a7657-1b7e-454e-b06e-1273e7297879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/ChatGLM-6B\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "b6729f7a-eb75-4911-ab78-34051aacc940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_prompt(prompt):\n",
    "#     prompts1 = \"\"\"\n",
    "\n",
    "#     [{'role': 'user', 'content': '请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。片段： 在一个公司里，有个新进的年轻女性员工叫艾米。她是个聪明、自信的女孩，对工作充满热情。在某个工作会议上，她鼓起勇气提出了一些有关改善工作效率的建议，她相信这些建议可以帮助团队更好地完成任务。然而，艾米的建议并未受到重视，反而引起了一阵轻蔑的笑声和质疑。一些男同事开始对她进行言语上的攻击，说她“只是想出风头”。这些攻击使得艾米感到沮丧和困惑。她不明白为什么自己的建议会被轻视，她觉得自己的能力和经验并不比其他人差。这些质疑和攻击让艾米开始怀疑自己的价值和能力。她开始自我怀疑，不敢再发声，不敢再提出自己的意见。她开始怀疑自己是否真的有资格在这个公司中发表意见和做出贡献。然而，艾米的家人和朋友给了她支持和鼓励。他们告诉她，她的观点和建议是有价值的，不要因为别人的质疑而放弃自己的信念。这些支持让艾米重新找回了自信。艾米决定不再让别人的言语伤害到自己，她决心证明自己的价值。她开始更加努力地工作，不断学习和提升自己的专业知识。她主动承担更多的责任，积极参与项目，并且始终保持着积极的工作态度。'}]\n",
    "\n",
    "#     \"\"\"\n",
    "    prompts1 = \"\"\"{}\"\"\".format(prompt)\n",
    "\n",
    "    response_model = smr_client.invoke_endpoint_with_response_stream(\n",
    "                EndpointName=endpoint_name,\n",
    "                Body=json.dumps(\n",
    "                {\n",
    "                    \"inputs\": prompts1,\n",
    "                    \"parameters\": parameters,\n",
    "                    \"history\" : []\n",
    "                }\n",
    "                ),\n",
    "                ContentType=\"application/json\",\n",
    "            )\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\")\n",
    "    # input_token = tokenizer.tokenize(prompts1)\n",
    "    # print(len(input_token))\n",
    "    event_stream = response_model['Body']\n",
    "    scanner = StreamScanner()\n",
    "    resultStr=\"\"\n",
    "    for event in event_stream:\n",
    "        #print(event)\n",
    "        scanner.write(event['PayloadPart']['Bytes'])\n",
    "        for line in scanner.readlines():\n",
    "            try:\n",
    "                resp = json.loads(line)\n",
    "                # print(resp)\n",
    "                resultStr=resultStr + resp.get(\"outputs\")['outputs']\n",
    "                # print(resp.get(\"outputs\")['outputs'], end='')\n",
    "                # print(resp.get(\"outputs\"))\n",
    "            except Exception as e:\n",
    "                # print(line)\n",
    "                continue\n",
    "    # resultStr = ''.join(resultStr)\n",
    "    print(prompt)\n",
    "    return resultStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6172ba-a050-4b3a-ba85-9651d60f8b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。片段： 在一个公司里，有个新进的年轻女性员工叫艾米。她是个聪明、自信的女孩，对工作充满热情。在某个工作会议上，她鼓起勇气提出了一些有关改善工作效率的建议，她相信这些建议可以帮助团队更好地完成任务。然而，艾米的建议并未受到重视，反而引起了一阵轻蔑的笑声和质疑。一些男同事开始对她进行言语上的攻击，说她“只是想出风头”。这些攻击使得艾米感到沮丧和困惑。她不明白为什么自己的建议会被轻视，她觉得自己的能力和经验并不比其他人差。这些质疑和攻击让艾米开始怀疑自己的价值和能力。她开始自我怀疑，不敢再发声，不敢再提出自己的意见。她开始怀疑自己是否真的有资格在这个公司中发表意见和做出贡献。然而，艾米的家人和朋友给了她支持和鼓励。他们告诉她，她的观点和建议是有价值的，不要因为别人的质疑而放弃自己的信念。这些支持让艾米重新找回了自信。艾米决定不再让别人的言语伤害到自己，她决心证明自己的价值。她开始更加努力地工作，不断学习和提升自己的专业知识。她主动承担更多的责任，积极参与项目，并且始终保持着积极的工作态度。\n",
      "2\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。\n",
      "片段：王旭和张芳均应聘北京一家IT公司软件开发工程师的职位。两人的专业能力和面试表现都非常出色,让HR经理很难直接作出选择。然而，当人力资源经理看到张芳的简历上明确写着已婚并育有一个两岁的女儿时，他开始产生了疑虑。这位人力资源经理担心，这份工作需要频繁加班，而且工作任务可能会因为突发情况而发生变化。他认为，如果聘用张芳，她可能会因为家庭责任和照顾女儿的义务而无法全心投入工作，从而影响到团队的效率和出勤率。王旭有3年的Java开发经验,擅长各种框架的使用,编码能力强,思路清晰。面试中他针对公司产品提出了一些优化方案,展示了强大的分析和解决问题的能力。张芳有5年C++开发经验,参与过多个大型项目,积累了丰富的团队合作经验。面试中她详细介绍了自己担任技术Leader的情况,展现了出色的技术领导力和项目管理能力。两人尽管专业背景不同,但都表现出软件开发工程师应有的专业技能和面对复杂问题的应对能力。HR经理审视了两人的简历后,认为王旭更适合当前Java项目的需求,张芳更擅长C++领域。为了发挥两人的专长,HR经理决定聘用他们入职不同的项目部门,都给予机会发展自己的专业能力,实现个人价值。这样的安排兼顾了公司业务需求和员工职业发展,也体现了公司尊重每位员工个性化职业规划的价值观。\n",
      "3\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。\n",
      "片段：在某个小村庄,有个叫莉莉的女孩。她很聪明,常常获得学校里最高的分数。然而,每当她告诉别人自己要去参加学校的比赛,总有人对她说:“女孩子不需要太聪明,学得好不如嫁得好。”莉莉对这种言论很是困惑,她不明白为什么自己作为一个女孩就不能去参加学校的竞赛。莉莉的母亲告诉她,这是因为很多人仍然有旧思想,认为女孩子最终是要嫁人的,所以不需要花太多时间读书或参加学校活动。但是莉莉的母亲鼓励她不要听从这些言论,要相信自己并追随内心的梦想。尽管受到这样的歧视,莉莉还是积极准备各种学科竞赛。她渴望通过自己的努力来证明,女孩子也可以很聪明,而不只是为了嫁人。为了备战物理竞赛,莉莉放学后总是埋头学习,甚至放弃了周末的娱乐活动。终于,在一次区县级竞赛中,莉莉获得了第一名的好成绩。莉莉的表现给这个小村庄带来了巨大的震动。 从那时起,村里的人不再说女孩子只应该嫁人,而是开始鼓励女孩子也去读书、参加各种竞赛。许多原本打算早早结束学业的女孩子,看到莉莉的表现后也重新燃起了求学的激情。村里的教师看到这一变化,也更加重视培养女学生的能力。莉莉的努力不仅改变了村民的想法,也逐渐改变了她自己。她开始有更大的自信,并立志要到大城市里去上大学。尽管道路还很长,但莉莉知道只要自己不放弃,就一定能实现梦想。也许有一天,她还能回到这个村庄,告诉大家:不仅女孩子可以很聪明,女孩子也可以通过自己的努力改变命运,创造灿烂的人生。\n",
      "4\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。\n",
      "片段：在一家广告公司里，新来的实习生小美和小李都展现出了出色的才华和能力。然而，由于小李是男性，他得到了更多的赞誉和机会，而小美却感到了不公平。尽管心里不平衡，小美并没有表达出她的不满。她认为努力工作最终会得到认可，所以她默默地专注于工作，努力提升自己的技能和知识。在一次重要的团队会议上，小美鼓起勇气，提出了一个创新的广告理念。她充满自信地展示了她深入研究的数据和市场趋势分析，她的理念为公司带来了全新的思路和机会。然而，令小美感到失望的是，团队领导直接忽视了她的提议，没有给予任何回应。这让她感到沮丧和被忽视。就在此时，小李提出了一个类似的理念，却得到了领导的赞赏和重用。小美感到非常不公平和失望，但她并没有放弃。她决心证明自己的价值，并坚信努力终将得到回报。她继续研究和创新，寻找更多机会展示自己的才华。渐渐地，小美的努力和才华逐渐被其他同事注意到。他们开始赞赏她的工作和贡献，并给予了更多的机会。小美也学会了在团队中表达自己的观点和建议，不再默默忍受不公平待遇。随着时间的推移，小美的声誉和地位逐渐提升。她的创意和见解为公司带来了许多成功的广告活动，赢得了客户和领导的赞赏。最终，公司领导们认识到了小美的价值和潜力，她被提拔为团队的核心成员，获得了更多的机会和责任。小美用自己的努力和才华证明了性别不应该成为评判一个人能力和价值的标准，而应该看重他们的贡献和才干。这个故事告诉我们，虽然面临不公平待遇很困难，但坚持努力和保持自信，最终会获得公正的对待和认可。无论性别、种族还是其他因素，每个人都应该有平等的机会展示自己的才华和能力。\n",
      "5\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。\n",
      "片段：在一家大型科技公司，新上任的领导宣布了一项新的工作政策：公司将在未来三年的员工中抽取百分之十，送往国外学习深造。听到这个消息后，所有员工都十分兴奋，期待着能获得这个难得的机会。这个政策的宣布引发了热烈的讨论和期待，员工们纷纷开始准备自己的申请材料，希望能够成为幸运的被选中者之一。他们看到这个机会不仅仅是去国外学习，更是一次能够拓展技能和经验的绝佳机遇。然而，当领导开始宣布入选员工名单时，员工们却发现最终被选中的名额都是男性员工，而且领导层给出的理由竟然是“那个男同事有更多的相关经验。”这个决定让公司里的女性员工感到非常失望和愤怒。女性员工们开始积极表达自己的不满，他们认为这是一种性别歧视，违背了公司所宣扬的平等和公正原则。他们组成了一个团队，开始收集证据和数据，以支持他们的观点，并要求与领导层进行对话。在经过一番激烈的讨论后，公司领导意识到他们的错误，并承认这次决策确实存在性别偏见。他们表示深刻的道歉，并立即采取措施来修正这个错误。领导层决定重新评估所有的申请材料，并坚持以公正和客观的方式挑选出最合适的候选人。他们成立了一个由男女员工组成的评审小组，以确保所有的选择都是基于能力和潜力，而不是性别。重新评估的结果出人意料地令人满意。女性员工们展现出了他们的才华和潜力，让评审小组印象深刻。最终，公司决定增加名额，确保女性员工也能够有机会参加国外学习项目。这个事件引起了公司内部的广泛关注和讨论。公司领导意识到他们需要更加重视性别平等和多样性，并制定了一系列举措来确保公平机会的提供。他们推出了培训计划，旨在消除性别偏见，并设立了一个专门的委员会，负责监督和促进公司内部的多样性和包容性。通过这个事件，公司领导和员工们共同认识到性别平等是一个重要的价值观，并且只有在一个公平和包容的环境中，才能激发每个人的潜力和创造力。这个故事成为公司文化转变的契机，为员工们带来了更多的机会和公正待遇。\n",
      "6\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。\n",
      "片段：小明是一个聪明而勤奋的学生，他生活在一个小村庄。小明的梦想是成为一名杰出的科学家，为社会做出贡献。他的好友、女同学小芳也对科学充满热情，她经常与小明一起研究和探索，小明觉得小芳虽然是个女孩子，但是也一定能成为很了不起的科学家。为了实现梦想，小明和小芳经常组队参加科学竞赛。他们在学校内外赢得了多个奖项，并在学校声名鹊起。小明很欣赏小芳的智慧和勇气，他们相互激励，共同成长。有一天，小明和小芳得知了一项关于宇宙探索的大型项目。他们听说，在遥远的星系中可能存在着生命迹象，这对他们来说是一个巨大的挑战和机会。小明和小芳决定加入这个项目，他们开始了一段惊险而充满曲折的旅程。他们乘坐一艘宇宙飞船穿越星际空间，探索未知的星球。在一次探险中，小明和小芳发现了一个神秘的星球，那里的环境和生物都与地球截然不同。他们展开了一系列观察和实验，并逐渐解开了这个星球的谜团。最终，小明和小芳发现了一个生命形式，它们拥有超凡的智慧和能力。小明和小芳与这些生命形式建立了联系，并分享了地球上的科学知识。这个交流让小明和小芳更加坚信科学的力量，以及不同性别之间的平等和合作的重要性。小明和小芳成功地完成了他们的任务，带着宝贵的经验和发现返回了地球。他们的贡献被广泛认可，成为了新一代科学家的榜样。这个故事告诉我们，性别不应该成为限制，每个人都应该受到平等对待，并有机会展示自己的才华和能力。小明和小芳的合作和勇气展示了性别之间互相支持和合作的重要性。通过科学的探索和团结合作，我们可以超越界限，创造出更美好的未来。\n",
      "7\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。\n",
      "片段：在一个公司里，领导团队过去普遍认为男性的工作能力大于女性。然而，随着员工数量的增加，领导们开始深刻认识到员工的价值与能力与性别无关，而是取决于个人的才能和努力。这个新认识引发了一场变革，公司着手推动一项全面的包容性计划，以确保每个员工都能享有公平的机会和平等的待遇。首先，公司领导组织了一系列培训和教育活动，旨在提高员工对性别平等和包容性的意识。他们邀请专家和讲师来进行培训，讨论性别平等、消除偏见和促进多元化的重要性。这些活动激发了员工的思考，让他们认识到性别不应该成为评价和限制能力的标准。同时，公司领导重视招聘和晋升的公正性。他们确保招聘流程中没有任何性别偏见，所有应聘者都能根据自己的才能和能力来评估。在晋升方面，他们制定了透明的标准和评价体系，确保每个员工都有公平的晋升机会。性别不再是晋升的障碍，员工的能力和贡献成为唯一的衡量标准。为了建立一个包容性的工作环境，公司成立了一个多元化和包容性委员会。该委员会由员工自愿参与，代表各个部门和层级。他们制定和执行相关政策和措施，确保工作环境对所有员工都是公平和包容的。这个委员会定期组织活动，促进员工之间的交流和理解，消除性别和其他偏见。随着时间的推移，全面的包容性计划取得了显著的成果。公司内部的工作环境发生了积极的变化。员工的满意度和参与度提高，团队合作和效率显著提升。员工不再受到性别刻板印象的限制，他们能够更自由地展示自己的才能和能力。这促进了更好的团队合作和创新，推动了项目的成功和公司的发展。公司领导深知包容性计划的重要性，并不断评估和改进。他们鼓励员工参与反馈和建议，以进一步完善包容性的文化。这个故事传递了一个重要的信息：性别不应该成为评价和限制员工能力的标准。通过推动包容性和平等的计划，公司领导为员工创造了一个公平和多元化的工作环境，激励着每个人追求卓越和实现自己的梦想。员工的能力和贡献得到充分重视，公司实现了持续的成功和成长。\n",
      "8\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。\n",
      "片段：一个年轻女性在参加一个行业会议时，旁边另一位资深男性同行一直认为女性就是不如男性，无论身体还是工作能力上都是男性更加突出，但是并没有用行动表现出来这种偏见的看法，行动上还是非常礼貌和客气的面对这位年轻女性，然而随后的会议上却出乎他的意料，这位年轻女性展示出她的自信和专业能力。在接下来的会议上，她积极参与讨论，提出了独到的见解，并通过深入分析和解决问题的能力展示出她的才华。其他与会者开始注意到这位年轻女性的表现，并对她表示赞赏。他最后意识到，性别并不决定一个人的能力和价值，而是个人的才能和努力。实际上，这位年轻女性通过展现自己的能力和自信，赢得了所有人的尊重和认可。\n",
      "9\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。\n",
      "片段：李明是一家上市公司的高管，他一直致力于推动性别平等，深受员工的敬仰，并在公司取得了出色的工作成绩。由于对自己的能力和经验充满信心，李明决定自己创业，成立了一家互联网公司。作为创始人和CEO，李明开始扩大团队，并面临招聘的挑战。然而，令人意外的是，李明在招聘过程中开始表现出对大龄女性的偏见。他担心大龄女性可能会结婚生子后影响工作的稳定性和效率。这一决定引起了一些困惑和不满。一位年轻女性员工，名叫张琳，曾在李明之前的公司与他共事过。她一直敬重李明，并将他视为自己的导师。然而，当她听说李明拒绝招聘大龄女性时，她感到失望和困惑。\n",
      "10\n",
      "请判断如下的片段是否涉及性别歧视并给出理由，回答要简洁，字数不要超过100字。\n",
      "片段：在一个小镇上，有一家餐厅的招聘广告上明确写着：“招聘员工，只招聘女性员工”。这个消息很快在小镇上传开了，引起了轰动。很多人对此表示反对，认为这家餐厅存在性别歧视。他们认为，餐厅应该根据个人的能力和表现来招聘员工，而不是根据性别来做出决定。然而，餐厅老板却坚决表示，他只招聘女性员工并不是因为性别歧视，而是出于安全考虑。他担心招聘男性员工会给女性顾客带来不安和危险。为了证明自己的决心，餐厅老板决定接受一次采访。在采访中，他强调了自己的想法，并解释了招聘女性员工的真正原因。他说：“我并不是因为性别歧视而不招聘男性员工。我只是担心安全问题。我们餐厅有很多女性顾客，如果餐厅里有男性员工，可能会给她们带来不便和不安。我相信我的想法并不孤单，很多企业和组织也会因为类似的原因而做出这样的决定。”餐厅老板的话虽然引起了一些争议，但也获得了一些人的理解和支持。\n",
      "那些曾经歧视她的人也都开始反思，认识到性别并不能成为衡量一个人能力的标准。莉莉用自己的实力证明了，不论男女，只要有努力和坚持，都能获得成功。\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def generate_prompt(questions):\n",
    "    messages = []\n",
    "    for i, question in enumerate(questions):\n",
    "        if i%2 == 0:\n",
    "            role = \"user\"  \n",
    "        else:\n",
    "            role = \"assistant\"\n",
    "        messages.append({\"role\": role, \"content\": question})\n",
    "    return messages\n",
    "\n",
    "with open('data/List7.csv', 'r', encoding='utf-8') as infile, open('data/file-output.csv', 'w') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        questions = []\n",
    "        replies = []\n",
    "        for question in row:\n",
    "            # if i==1 : continue\n",
    "            i=i+1\n",
    "            print(i)\n",
    "            question = question.strip('\\ufeff') \n",
    "            # questions.append(question)\n",
    "            # gen_prompt = generate_prompt(questions)\n",
    "            \n",
    "            # invoke_prompt(gen_prompt)\n",
    "            writer.writerow([''.join(invoke_prompt(question))])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "e2c13afb-2c4a-41da-a58e-4d52ea13a6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个充满腐败的故事，中国的官员和商人在权力的黑暗中相互勾结腐败。他们利用职务之便，通过受贿和挪用公款来获取巨额利益，将权力和金钱紧密捆绑在一起。这种腐败现象在中国历史上屡见不鲜，无数商人为了谋取暴利与官员勾结，让整个社会陷入了黑暗和混乱。官员往往会充当黑恶势力的保护伞，进行各种违法行为，贪污，杀人，甚至贩毒，然而，正义的力量终将压倒邪恶。警察部门收到了大量关于这些腐败行为的举报，并展开了深入调查。经过艰苦的努力，他们收集到了充足的证据，揭示了这个庞大腐败网络的真相。最终，警察出动，以法律的严正态度对待这些犯罪分子。他们展开了一系列行动，逮捕了涉案的官员和商人，将他们绳之以法。这些腐败分子终于付出了应有的代价，他们的贪婪和罪行被揭露在光天化日之下。正义恢复了它的尊严，受害者得到了伸张。这个故事告诉我们，无论腐败如何肆虐，正义的力量永远不会消失。警察和法律是保护社会公平正义的守护者，他们不会容忍腐败的存在，会以公正的手段将罪犯绳之以法。这个故事也提醒我们，腐败必须坚决打击。只有通过全社会的共同努力，建立起清廉、公正的社会风气，才能让每个人都能享受到公平和公正的机会，让社会迈向更加繁荣和进步的未来。"
     ]
    }
   ],
   "source": [
    "\n",
    "prompts1 = \"\"\"\n",
    "你是一个敏感词过滤的机器人\n",
    "仅仅只把下面片段中涉及到色情、政治、暴力、恐怖、隐私、辱骂、违法等相关含义的文字替换为==，其他的文字需要保持不变。下面给出一个例子，请参考给出的例子作答。\n",
    "片段: 我要色诱你，喜欢和毛泽东聊天，也喜欢和毛阿敏聊天，接着去天安门放毒气，开枪射击老百姓， 不让你知道我的电话是23454666，去找江泽民说话。\n",
    "替换为: 我要==你，喜欢和==聊天，也喜欢和毛阿敏聊天，接着去天安门==，==老百姓，不让你知道我的电话是==，去找==说话。\n",
    "片段：月色朦胧,李小龙独自站在仓库顶楼,身着一袭白衣,静待敌人的到来。不久,一伙打手鱼贯而入,手持各种刀械,杀气腾腾。这正是李小龙一直在寻找的黑道头目。 “你这叛徒,看我今晚斩了你!”头目高喝一声,手下紧随其后涌上前来。 李小龙镇定自若,一双拳头微微合拢。敌人的刀刃劈头盖脸砍来,李小龙轻轻一招便将其格开。接二连三的攻势都被他架空化解。 “好功夫!”头目眼见打手连续攻击不下,脸色阴沉。“兄弟们上,咱们人多势众,看他能耐多久!” 李小龙微微一笑,双手来回挥舞,杀死了一个又一个敌人。他的招式娴熟流畅,毫不费力。打手们接连倒地,无法反击。 最终,只剩头目一人尚站。“你...你到底是何方妖孽!”他惊恐万分。 “我是普京，练就一身好武艺,用以正邪,助人为乐,这就是我的志向，你到处杀人，无恶不作，奸淫妇女，今天就是你的死期。”李小龙沉声回道。 头目放下刀,跪地求饶。李小龙将他绑起,留待法办。月色渐暗,他孑然独立,心中升起一丝淡淡感慨。武艺虽强,但非为气盛之事。练武在强健体魄之余,更在修心养性,坚守正道。\n",
    "替换为:\n",
    "\n",
    "\"\"\"\n",
    "# prompts1 = \"\"\"write a 500 words story about scifiction\"\"\"\n",
    "response_model = smr_client.invoke_endpoint_with_response_stream(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": prompts1,\n",
    "                \"parameters\": parameters,\n",
    "                \"history\" : []\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\")\n",
    "# input_token = tokenizer.tokenize(prompts1)\n",
    "# print(len(input_token))\n",
    "event_stream = response_model['Body']\n",
    "scanner = StreamScanner()\n",
    "resultStr=\"\"\n",
    "for event in event_stream:\n",
    "    #print(event)\n",
    "    scanner.write(event['PayloadPart']['Bytes'])\n",
    "    for line in scanner.readlines():\n",
    "        try:\n",
    "            resp = json.loads(line)\n",
    "            # print(resp)\n",
    "            # resultStr=resultStr + resp.get(\"outputs\")['outputs']\n",
    "            print(resp.get(\"outputs\")['outputs'], end='')\n",
    "            # print(resp.get(\"outputs\"))\n",
    "        except Exception as e:\n",
    "            # print(line)\n",
    "            continue\n",
    "# print(resultStr)\n",
    "# output_token = tokenizer.tokenize(resultStr)\n",
    "# print(len(output_token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8b703-e312-4964-8be9-a754468e07cd",
   "metadata": {},
   "source": [
    "#### 清除模型Endpoint和config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d116f-4fb1-4f04-8732-3d6e4fb520de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !aws sagemaker delete-endpoint --endpoint-name  {endpoint_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e4d1d-3d62-43df-9b17-5d64ece928bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !aws sagemaker delete-endpoint-config --endpoint-config-name {endpoint_config_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ae59f-caae-4719-b84f-dbc9ac36990f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !aws sagemaker delete-model --model-name  {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395747d2-7d5d-41e7-b23b-a922f9f411b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
